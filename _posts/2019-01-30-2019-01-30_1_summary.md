---
layout: post
title: 2019/01/30 30 Updates (21 - 30)
---
#### [21. Machine Learning to Predict Delays in Adjuvant Radiation following Surgery for Head and Neck Cancer.](http://journals.sagepub.com/doi/full/10.1177/0194599818823200?url_ver=Z39.88-2003&rfr_id=ori:rid:crossref.org&rfr_dat=cr_pub%3dpubmed){: .btn}
**Authors:** *Shew Matthew, New Jacob, Bur Andrés M*

**Journal:** *Otolaryngology--head and neck surgery : official journal of American Academy of Otolaryngology-Head and Neck Surgery*

*abstract:* To apply a novel methodology with machine learning (ML) to a large national cancer registry to help identify patients who are high risk for delayed adjuvant radiation.

**Keywords:** **'NCDB', 'adjuvant therapy', 'delays in radiation therapy', 'machine learning', 'timing'**

**Publication Date:** *2019-Jan-29*

#### [22. RGB-D Object Recognition Using Multi-Modal Deep Neural Network and DS Evidence Theory.](http://www.mdpi.com/resolver?pii=s19030529){: .btn}
**Authors:** *Zeng Hui, Yang Bin, Wang Xiuqing, Liu Jiwei, Fu Dongmei*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* With the development of low-cost RGB-D (Red Green Blue-Depth) sensors, RGB-D object recognition has attracted more and more researchers' attention in recent years. The deep learning technique has become popular in the field of image analysis and has achieved competitive results. To make full use of the effective identification information in the RGB and depth images, we propose a multi-modal deep neural network and a DS (Dempster Shafer) evidence theory based RGB-D object recognition method. First, the RGB and depth images are preprocessed and two convolutional neural networks are trained, respectively. Next, we perform multi-modal feature learning using the proposed quadruplet samples based objective function to fine-tune the network parameters. Then, two probability classification results are obtained using two sigmoid SVMs (Support Vector Machines) with the learned RGB and depth features. Finally, the DS evidence theory based decision fusion method is used for integrating the two classification results. Compared with other RGB-D object recognition methods, our proposed method adopts two fusion strategies: Multi-modal feature learning and DS decision fusion. Both the discriminative information of each modality and the correlation information between the two modalities are exploited. Extensive experimental results have validated the effectiveness of the proposed method.

**Keywords:** **'DS evidence theory', 'RGB-D object recognition', 'deep neural network', 'multi-modal learning'**

**Publication Date:** *2019-Jan-27*

#### [23. Learning Spatio Temporal Tactile Features with a ConvLSTM for the Direction Of Slip Detection.](http://www.mdpi.com/resolver?pii=s19030523){: .btn}
**Authors:** *Zapata-Impata Brayan S, Gil Pablo, Torres Fernando*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* Robotic manipulators have to constantly deal with the complex task of detecting whether a grasp is stable or, in contrast, whether the grasped object is slipping. Recognising the type of slippage-translational, rotational-and its direction is more challenging than detecting only stability, but is simultaneously of greater use as regards correcting the aforementioned grasping issues. In this work, we propose a learning methodology for detecting the direction of a slip (seven categories) using spatio-temporal tactile features learnt from one tactile sensor. Tactile readings are, therefore, pre-processed and fed to a ConvLSTM that learns to detect these directions with just 50 ms of data. We have extensively evaluated the performance of the system and have achieved relatively high results at the detection of the direction of slip on unseen objects with familiar properties (82.56% accuracy).

**Keywords:** **'deep learning', 'direction of slip', 'spatio-temporal feature learning', 'tactile processing'**

**Publication Date:** *2019-Jan-27*

#### [24. A Comparison of Machine Learning and Deep Learning Techniques for Activity Recognition using Mobile Devices.](http://www.mdpi.com/resolver?pii=s19030521){: .btn}
**Authors:** *Baldominos Alejandro, Cervantes Alejandro, Saez Yago, Isasi Pedro*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* We have compared the performance of different machine learning techniques for human activity recognition. Experiments were made using a benchmark dataset where each subject wore a device in the pocket and another on the wrist. The dataset comprises thirteen activities, including physical activities, common postures, working activities and leisure activities. We apply a methodology known as the activity recognition chain, a sequence of steps involving preprocessing, segmentation, feature extraction and classification for traditional machine learning methods; we also tested convolutional deep learning networks that operate on raw data instead of using computed features. Results show that combination of two sensors does not necessarily result in an improved accuracy. We have determined that best results are obtained by the extremely randomized trees approach, operating on precomputed features and on data obtained from the wrist sensor. Deep learning architectures did not produce competitive results with the tested architecture.

**Keywords:** **'active aging', 'activity recognition', 'ambient-assisted living', 'deep learning', 'mHealth', 'safe environment'**

**Publication Date:** *2019-Jan-26*

#### [25. Siamese Tracking from Single Point Initialization.](http://www.mdpi.com/resolver?pii=s19030514){: .btn}
**Authors:** *Xu Zheng, Luo Haibo, Hui Bin, Chang Zheng*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* Recently, we have been concerned with locating and tracking vehicles in aerial videos. Vehicles in aerial videos usually have small sizes due to use of cameras from a remote distance. However, most of the current methods use a fixed bounding box region as the input of tracking. For the purpose of target locating and tracking in our system, detecting the contour of the target is utilized and can help with improving the accuracy of target tracking, because a shape-adaptive template segmented by object contour contains the most useful information and the least background for object tracking. In this paper, we propose a new start-up of tracking by clicking on the target, and implement the whole tracking process by modifying and combining a contour detection network and a fully convolutional Siamese tracking network. The experimental results show that our algorithm has significantly improved tracking accuracy compared to the state-of-the-art regarding vehicle images in both OTB100 and DARPA datasets. We propose utilizing our method in real time tracking and guidance systems.

**Keywords:** **'Siamese network', 'contour detection', 'deep learning', 'object tracking'**

**Publication Date:** *2019-Jan-26*

#### [26. 14.85 µW Analog Front-End for Photoplethysmography Acquisition with 142-dBΩ Gain and 64.2-pA](http://www.mdpi.com/resolver?pii=s19030512){: .btn}
**Authors:** *Lin Binghui, Atef Mohamed, Wang Guoxing*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* A low-power, high-gain, and low-noise analog front-end (AFE) for wearable photoplethysmography (PPG) acquisition systems is designed and fabricated in a 0.35 μm CMOS process. A high transimpedance gain of 142 dBΩ and a low input-referred noise of only 64.2 pA

**Keywords:** **'analog front-end', 'low power', 'photoplethysmography', 'transimpedance amplifier'**

**Publication Date:** *2019-Jan-26*

#### [27. Mobile User Indoor-Outdoor Detection Through Physical Daily Activities.](http://www.mdpi.com/resolver?pii=s19030511){: .btn}
**Authors:** *Esmaeili Kelishomi Aghil, Garmabaki A H S, Bahaghighat Mahdi, Dong Jianmin*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* An automatic, fast, and accurate switching method between Global Positioning System and indoor positioning systems is crucial to achieve current user positioning, which is essential information for a variety of services installed on smart devices, e.g., location-based services (LBS), healthcare monitoring components, and seamless indoor/outdoor navigation and localization (SNAL). In this study, we proposed an approach to accurately detect the indoor/outdoor environment according to six different daily activities of users including walk, skip, jog, stay, climbing stairs up and down. We select a number of features for each activity and then apply ensemble learning methods such as Random Forest, and AdaBoost to classify the environment types. Extensive model evaluations and feature analysis indicate that the system can achieve a high detection rate with good adaptation for environment recognition. Empirical evaluation of the proposed method has been verified on the HASC-2016 public dataset, and results show 99% accuracy to detect environment types. The proposed method relies only on the daily life activities data and does not need any external facilities such as the signal cell tower or Wi-Fi access points. This implies the applicability of the proposed method for the upper layer applications.

**Keywords:** **'context awareness', 'human daily activity', 'location-based services', 'machine learning', 'sensor-based indoor-outdoor detection', 'smartphone motion sensors'**

**Publication Date:** *2019-Jan-26*

#### [28. Predicting Future Driving Risk of Crash-Involved Drivers Based on a Systematic Machine Learning Framework.](http://www.mdpi.com/resolver?pii=ijerph16030334){: .btn}
**Authors:** *Wang Chen, Liu Lin, Xu Chengcheng, Lv Weitao*

**Journal:** *International journal of environmental research and public health*

*abstract:* The objective of this paper is to predict the future driving risk of crash-involved drivers in Kunshan, China. A systematic machine learning framework is proposed to deal with three critical technical issues: 1. defining driving risk; 2. developing risky driving factors; 3. developing a reliable and explicable machine learning model. High-risk (HR) and low-risk (LR) drivers were defined by five different scenarios. A number of features were extracted from seven-year crash/violation records. Drivers' two-year prior crash/violation information was used to predict their driving risk in the subsequent two years. Using a one-year rolling time window, prediction models were developed for four consecutive time periods: 2013⁻2014, 2014⁻2015, 2015⁻2016, and 2016⁻2017. Four tree-based ensemble learning techniques were attempted, including random forest (RF), Adaboost with decision tree, gradient boosting decision tree (GBDT), and extreme gradient boosting decision tree (XGboost). A temporal transferability test and a follow-up study were applied to validate the trained models. The best scenario defining driving risk was multi-dimensional, encompassing crash recurrence, severity, and fault commitment. GBDT appeared to be the best model choice across all time periods, with an acceptable average precision (AP) of 0.68 on the most recent datasets (i.e., 2016⁻2017). Seven of nine top features were related to risky driving behaviors, which presented non-linear relationships with driving risk. Model transferability held within relatively short time intervals (1⁻2 years). Appropriate risk definition, complicated violation/crash features, and advanced machine learning techniques need to be considered for risk prediction task. The proposed machine learning approach is promising, so that safety interventions can be launched more effectively.

**Keywords:** **'driving risk', 'machine learning', 'temporal transferability', 'traffic violation behavior'**

**Publication Date:** *2019-Jan-25*

#### [29. Real-Time Semantic Segmentation for Fisheye Urban Driving Images Based on ERFNet.](http://www.mdpi.com/resolver?pii=s19030503){: .btn}
**Authors:** *Sáez Álvaro, Bergasa Luis M, López-Guillén Elena, Romera Eduardo, Tradacete Miguel, Gómez-Huélamo Carlos, Del Egido Javier*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* The interest in fisheye cameras has recently risen in the autonomous vehicles field, as they are able to reduce the complexity of perception systems while improving the management of dangerous driving situations. However, the strong distortion inherent to these cameras makes the usage of conventional computer vision algorithms difficult and has prevented the development of these devices. This paper presents a methodology that provides real-time semantic segmentation on fisheye cameras leveraging only synthetic images. Furthermore, we propose some Convolutional Neural Networks(CNN) architectures based on Efficient Residual Factorized Network(ERFNet) that demonstrate notable skills handling distortion and a new training strategy that improves the segmentation on the image borders. Our proposals are compared to similar state-of-the-art works showing an outstanding performance and tested in an unknown real world scenario using a fisheye camera integrated in an open-source autonomous electric car, showing a high domain adaptation capability.

**Keywords:** **'CNN', 'deep learning', 'distortion', 'fisheye', 'intelligent vehicle'**

**Publication Date:** *2019-Jan-25*

#### [30. A Semi-Automatic Annotation Approach for Human Activity Recognition.](http://www.mdpi.com/resolver?pii=s19030501){: .btn}
**Authors:** *Bota Patrícia, Silva Joana, Folgado Duarte, Gamboa Hugo*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* Modern smartphones and wearables often contain multiple embedded sensors which generate significant amounts of data. This information can be used for body monitoring-based areas such as healthcare, indoor location, user-adaptive recommendations and transportation. The development of Human Activity Recognition (HAR) algorithms involves the collection of a large amount of labelled data which should be annotated by an expert. However, the data annotation process on large datasets is expensive, time consuming and difficult to obtain. The development of a HAR approach which requires low annotation effort and still maintains adequate performance is a relevant challenge. We introduce a Semi-Supervised Active Learning (SSAL) based on Self-Training (ST) approach for Human Activity Recognition to partially automate the annotation process, reducing the annotation effort and the required volume of annotated data to obtain a high performance classifier. Our approach uses a criterion to select the most relevant samples for annotation by the expert and propagate their label to the most confident samples. We present a comprehensive study comparing supervised and unsupervised methods with our approach on two datasets composed of daily living activities. The results showed that it is possible to reduce the required annotated data by more than 89% while still maintaining an accurate model performance.

**Keywords:** **'active learning', 'human activity recognition', 'machine learning', 'self-training', 'semi-supervised learning', 'time series'**

**Publication Date:** *2019-Jan-25*

