---
layout: post
title: 2019/01/30 34 Updates (31 - 34)
---
#### [31. Mobile User Indoor-Outdoor Detection Through Physical Daily Activities.](http://www.mdpi.com/resolver?pii=s19030511){: .btn}
**Authors:** *Esmaeili Kelishomi Aghil, Garmabaki A H S, Bahaghighat Mahdi, Dong Jianmin*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* An automatic, fast, and accurate switching method between Global Positioning System and indoor positioning systems is crucial to achieve current user positioning, which is essential information for a variety of services installed on smart devices, e.g., location-based services (LBS), healthcare monitoring components, and seamless indoor/outdoor navigation and localization (SNAL). In this study, we proposed an approach to accurately detect the indoor/outdoor environment according to six different daily activities of users including walk, skip, jog, stay, climbing stairs up and down. We select a number of features for each activity and then apply ensemble learning methods such as Random Forest, and AdaBoost to classify the environment types. Extensive model evaluations and feature analysis indicate that the system can achieve a high detection rate with good adaptation for environment recognition. Empirical evaluation of the proposed method has been verified on the HASC-2016 public dataset, and results show 99% accuracy to detect environment types. The proposed method relies only on the daily life activities data and does not need any external facilities such as the signal cell tower or Wi-Fi access points. This implies the applicability of the proposed method for the upper layer applications.

**Keywords:** **'context awareness', 'human daily activity', 'location-based services', 'machine learning', 'sensor-based indoor-outdoor detection', 'smartphone motion sensors'**

**Publication Date:** *2019-Jan-26*

#### [32. Predicting Future Driving Risk of Crash-Involved Drivers Based on a Systematic Machine Learning Framework.](http://www.mdpi.com/resolver?pii=ijerph16030334){: .btn}
**Authors:** *Wang Chen, Liu Lin, Xu Chengcheng, Lv Weitao*

**Journal:** *International journal of environmental research and public health*

*abstract:* The objective of this paper is to predict the future driving risk of crash-involved drivers in Kunshan, China. A systematic machine learning framework is proposed to deal with three critical technical issues: 1. defining driving risk; 2. developing risky driving factors; 3. developing a reliable and explicable machine learning model. High-risk (HR) and low-risk (LR) drivers were defined by five different scenarios. A number of features were extracted from seven-year crash/violation records. Drivers' two-year prior crash/violation information was used to predict their driving risk in the subsequent two years. Using a one-year rolling time window, prediction models were developed for four consecutive time periods: 2013⁻2014, 2014⁻2015, 2015⁻2016, and 2016⁻2017. Four tree-based ensemble learning techniques were attempted, including random forest (RF), Adaboost with decision tree, gradient boosting decision tree (GBDT), and extreme gradient boosting decision tree (XGboost). A temporal transferability test and a follow-up study were applied to validate the trained models. The best scenario defining driving risk was multi-dimensional, encompassing crash recurrence, severity, and fault commitment. GBDT appeared to be the best model choice across all time periods, with an acceptable average precision (AP) of 0.68 on the most recent datasets (i.e., 2016⁻2017). Seven of nine top features were related to risky driving behaviors, which presented non-linear relationships with driving risk. Model transferability held within relatively short time intervals (1⁻2 years). Appropriate risk definition, complicated violation/crash features, and advanced machine learning techniques need to be considered for risk prediction task. The proposed machine learning approach is promising, so that safety interventions can be launched more effectively.

**Keywords:** **'driving risk', 'machine learning', 'temporal transferability', 'traffic violation behavior'**

**Publication Date:** *2019-Jan-25*

#### [33. Real-Time Semantic Segmentation for Fisheye Urban Driving Images Based on ERFNet.](http://www.mdpi.com/resolver?pii=s19030503){: .btn}
**Authors:** *Sáez Álvaro, Bergasa Luis M, López-Guillén Elena, Romera Eduardo, Tradacete Miguel, Gómez-Huélamo Carlos, Del Egido Javier*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* The interest in fisheye cameras has recently risen in the autonomous vehicles field, as they are able to reduce the complexity of perception systems while improving the management of dangerous driving situations. However, the strong distortion inherent to these cameras makes the usage of conventional computer vision algorithms difficult and has prevented the development of these devices. This paper presents a methodology that provides real-time semantic segmentation on fisheye cameras leveraging only synthetic images. Furthermore, we propose some Convolutional Neural Networks(CNN) architectures based on Efficient Residual Factorized Network(ERFNet) that demonstrate notable skills handling distortion and a new training strategy that improves the segmentation on the image borders. Our proposals are compared to similar state-of-the-art works showing an outstanding performance and tested in an unknown real world scenario using a fisheye camera integrated in an open-source autonomous electric car, showing a high domain adaptation capability.

**Keywords:** **'CNN', 'deep learning', 'distortion', 'fisheye', 'intelligent vehicle'**

**Publication Date:** *2019-Jan-25*

#### [34. A Semi-Automatic Annotation Approach for Human Activity Recognition.](http://www.mdpi.com/resolver?pii=s19030501){: .btn}
**Authors:** *Bota Patrícia, Silva Joana, Folgado Duarte, Gamboa Hugo*

**Journal:** *Sensors (Basel, Switzerland)*

*abstract:* Modern smartphones and wearables often contain multiple embedded sensors which generate significant amounts of data. This information can be used for body monitoring-based areas such as healthcare, indoor location, user-adaptive recommendations and transportation. The development of Human Activity Recognition (HAR) algorithms involves the collection of a large amount of labelled data which should be annotated by an expert. However, the data annotation process on large datasets is expensive, time consuming and difficult to obtain. The development of a HAR approach which requires low annotation effort and still maintains adequate performance is a relevant challenge. We introduce a Semi-Supervised Active Learning (SSAL) based on Self-Training (ST) approach for Human Activity Recognition to partially automate the annotation process, reducing the annotation effort and the required volume of annotated data to obtain a high performance classifier. Our approach uses a criterion to select the most relevant samples for annotation by the expert and propagate their label to the most confident samples. We present a comprehensive study comparing supervised and unsupervised methods with our approach on two datasets composed of daily living activities. The results showed that it is possible to reduce the required annotated data by more than 89% while still maintaining an accurate model performance.

**Keywords:** **'active learning', 'human activity recognition', 'machine learning', 'self-training', 'semi-supervised learning', 'time series'**

**Publication Date:** *2019-Jan-25*

