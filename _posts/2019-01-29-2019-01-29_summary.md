---
layout: post
title: "2019/01/29 40 Updates"
---
## 1. Artificial intelligence and machine learning for human reproduction and embryology presented at ASRM and ESHRE 2018.
**Keywords:** **'ASHRE', 'ASRM', 'Artificial intelligence', 'Embryology', 'Human reproduction', 'Machine learning'**

*Journal: Journal of assisted reproduction and genetics* *Publication Date: 2019-Jan-28*

[https://doi.org/10.1007/s10815-019-01408-x](https://doi.org/10.1007/s10815-019-01408-x){: .btn}

*abstract:* Sixteen artificial intelligence (AI) and machine learning (ML) approaches were reported at the 2018 annual congresses of the American Society for Reproductive Biology (9) and European Society for Human Reproduction and Embryology (7). Nearly every aspect of patient care was investigated, including sperm morphology, sperm identification, identification of empty or oocyte containing follicles, predicting embryo cell stages, predicting blastocyst formation from oocytes, assessing human blastocyst quality, predicting live birth from blastocysts, improving embryo selection, and for developing optimal IVF stimulation protocols. This represents a substantial increase in reports over 2017, where just one abstract each was reported at ASRM (AI) and ESHRE (ML). Our analysis reveals wide variability in how AI and ML methods are described (from not at all or very generic to fully describing the architectural framework) and large variability on accepted dataset sizes (from just 3 patients with 16 follicles in the smallest dataset to 661,060 images of 11,898 human embryos in one of the largest). AI and ML are clearly burgeoning methodologies in human reproduction and embryology and would benefit from early application of reporting standards.

## 2. Assessment of advanced random forest and decision tree algorithms for modeling rainfall-induced landslide susceptibility in the Izu-Oshima Volcanic Island, Japan.
**Keywords:** **'Decision tree', 'Izu-Oshima Volcano Island', 'Machine learning', 'Rainfall-induced landslide', 'Random forest', 'Susceptibility'**

*Journal: The Science of the total environment* *Publication Date: 2019-Jan-21*

[https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(19)30305-5](https://linkinghub.elsevier.com/retrieve/pii/S0048-9697(19)30305-5){: .btn}

*abstract:* Landslides represent a part of the cascade of geological hazards in a wide range of geo-environments. In this study, we aim to investigate and compare the performance of two state-of-the-art machine learning models, i.e., decision tree (DT) and random forest (RF) approaches to model the massive rainfall-triggered landslide occurrences in the Izu-Oshima Volcanic Island, Japan at a regional scale. At first, a landslide inventory map is prepared consisting of 44 landslide polygons (10,444 pixels) from aerial photo-interpretation and field surveys. To estimate the robustness of the models, we randomly adapted two different samples (S1 and S2), comprising of both positive and negative cells (70% of total landslides - 7293 pixels) for training and remaining (30%-3151 pixels) for validation. Twelve causative factors including altitude, slope angle, slope aspect, plan curvature, total curvature, compound topographic index, stream power index, distance to drainage network, drainage density, distance to geological boundaries, lithology and cumulative rainfall were selected as predictors to implement the landslide susceptibility model. The area under the receiver operating characteristics (ROC) curves (AUC) and other statistical signifiers were used to verify the model accuracies. The result shows that the DT and RF models achieved remarkable predictive performance (AUC > 0.9), producing near accurate susceptibility maps. The overall efficiency of RF (AUC = 0.956) is found significantly higher than the DT (AUC = 0.928) results. Additionally, we noticed that the performance of RF for modeling landslide susceptibility is very robust even though the training and validation samples are altered. Considering the performances, we suggest that both RF and DT models can be used in other similar non-eruption-related landslide studies in the tephra-deposited rich volcanoes, as they are capable of rapidly generating accurate and stable LSM maps for risk mitigation, management practices, and decision-making. Moreover, the RF-based model is promising and enough to be recommended as a method to map regional landslide susceptibility.

## 3. Image-based velocity estimation of rock using Convolutional Neural Networks.
**Keywords:** **'Artificial intelligence', 'Digital Rock Physics (DRP)', 'HYPPS', 'P- and S-wave velocities'**

*Journal: Neural networks : the official journal of the International Neural Network Society* *Publication Date: 2019-Jan-09*

[https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(18)30337-X](https://linkinghub.elsevier.com/retrieve/pii/S0893-6080(18)30337-X){: .btn}

*abstract:* Digital images of rock samples have been using extensively in Digital Rock Physics (DRP) to evaluate physical parameters of rock such as permeability, P- and S-wave velocities and formation factor. The parameters are numerically computed by simulation of the corresponding physical processes through segmented image of rock, which provide a direct and accurate evaluation of rock properties. However, recent advances in machine learning and Convolutional Neural Networks (CNN) allow using images as input. Such networks, however, require a considerable number of images as input. In this paper, CNNs are used to estimate the P- and S-wave velocities from images of rock medium. To deal with lack of input data, a hybrid pattern- and pixel-based simulation (HYPPS) is used as an efficient data augmentation method to increase the training data set. For each input image, 10 stochastic realizations are produced. Compare to the case wherein the stochastic models are not used, the new results from the enhanced network indicate a sharp improvement in the estimations such that R

## 4. Implementation and relevance of FAIR data principles in biopharmaceutical R&D.


*Journal: Drug discovery today* *Publication Date: 2019-Jan-25*

[https://linkinghub.elsevier.com/retrieve/pii/S1359-6446(18)30303-9](https://linkinghub.elsevier.com/retrieve/pii/S1359-6446(18)30303-9){: .btn}

*abstract:* Biopharmaceutical industry R&D, and indeed other life sciences R&D such as biomedical, environmental, agricultural and food production, is becoming increasingly data-driven and can significantly improve its efficiency and effectiveness by implementing the FAIR (findable, accessible, interoperable, reusable) guiding principles for scientific data management and stewardship. By so doing, the plethora of new and powerful analytical tools such as artificial intelligence and machine learning will be able, automatically and at scale, to access the data from which they learn, and on which they thrive. FAIR is a fundamental enabler for digital transformation.

## 5. Intracortical smoothing of small-voxel fMRI data can provide increased detection power without spatial resolution losses compared to conventional large-voxel fMRI data.
**Keywords:** **'Columnar fMRI', 'Cortical depth analysis', 'High-resolution fMRI', 'Laminar fMRI', 'Physiological noise', 'Spatial smoothing', 'Surface-based analysis', 'fMRI analysis'**

*Journal: NeuroImage* *Publication Date: 2019-Jan-25*

[https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(19)30048-5](https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(19)30048-5){: .btn}

*abstract:* Continued improvement in MRI acquisition technology has made functional MRI (fMRI) with small isotropic voxel sizes down to 1 mm and below more commonly available. Although many conventional fMRI studies seek to investigate regional patterns of cortical activation for which conventional voxel sizes of 3 mm and larger provide sufficient spatial resolution, smaller voxels can help avoid contamination from adjacent white matter (WM) and cerebrospinal fluid (CSF), and thereby increase the specificity of fMRI to signal changes within the gray matter. Unfortunately, temporal signal-to-noise ratio (tSNR), a metric of fMRI sensitivity, is reduced in high-resolution acquisitions, which offsets the benefits of small voxels. Here we introduce a framework that combines small, isotropic fMRI voxels acquired at 7 T field strength with a novel anatomically-informed, surface mesh-navigated spatial smoothing that can provide both higher detection power and higher resolution than conventional voxel sizes. Our smoothing approach uses a family of intracortical surface meshes and allows for kernels of various shapes and sizes, including curved 3D kernels that adapt to and track the cortical folding pattern. Our goal is to restrict smoothing to the cortical gray matter ribbon and avoid noise contamination from CSF and signal dilution from WM via partial volume effects. We found that the intracortical kernel that maximizes tSNR does not maximize percent signal change (ΔS/S), and therefore the kernel configuration that optimizes detection power cannot be determined from tSNR considerations alone. However, several kernel configurations provided a favorable balance between boosting tSNR and ΔS/S, and allowed a 1.1-mm isotropic fMRI acquisition to have higher performance after smoothing (in terms of both detection power and spatial resolution) compared to an unsmoothed 3.0-mm isotropic fMRI acquisition. Overall, the results of this study support the strategy of acquiring voxels smaller than the cortical thickness, even for studies not requiring high spatial resolution, and smoothing them down within the cortical ribbon with a kernel of an appropriate shape to achieve the best performance-thus decoupling the choice of fMRI voxel size from the spatial resolution requirements of the particular study. The improvement of this new intracortical smoothing approach over conventional surface-based smoothing is expected to be modest for conventional resolutions, however the improvement is expected to increase with higher resolutions. This framework can also be applied to anatomically-informed intracortical smoothing of higher-resolution data (e.g. along columns and layers) in studies with prior information about the spatial structure of activation.

## 6. A Deep Learning Algorithm to Quantify Neuroretinal Rim Loss from Optic Disc Photographs.


*Journal: American journal of ophthalmology* *Publication Date: 2019-Jan-25*

[https://linkinghub.elsevier.com/retrieve/pii/S0002-9394(19)30024-8](https://linkinghub.elsevier.com/retrieve/pii/S0002-9394(19)30024-8){: .btn}

*abstract:* To train a deep learning (DL) algorithm that quantifies glaucomatous neuroretinal damage on fundus photographs using the minimum rim width relative to Bruch's membrane opening (BMO-MRW) from spectral domain-optical coherence tomography (SDOCT).

## 7. A new method for detecting autocorrelation of evolutionary rates in large phylogenies.


*Journal: Molecular biology and evolution* *Publication Date: 2019-Jan-23*

[https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msz014](https://academic.oup.com/mbe/article-lookup/doi/10.1093/molbev/msz014){: .btn}

*abstract:* New species arise from pre-existing species and inherit similar genomes and environments. This predicts greater similarity of the tempo of molecular evolution between direct ancestors and descendants, resulting in autocorrelation of evolutionary rates in the tree of life. Surprisingly, molecular sequence data have not confirmed this expectation, possibly because available methods lack the power to detect autocorrelated rates. Here we present a machine learning method, CorrTest, to detect the presence of rate autocorrelation in large phylogenies. CorrTest is computationally efficient and performs better than the available state-of-the-art methods. Application of CorrTest reveals extensive rate autocorrelation in DNA and amino acid sequence evolution of mammals, birds, insects, metazoans, plants, fungi, parasitic protozoans, and prokaryotes. Therefore, rate autocorrelation is a common phenomenon throughout the tree of life. These findings suggest concordance between molecular and non-molecular evolutionary patterns, and they will foster unbiased and precise dating of the tree of life.

## 8. NanoPipe - a web server for nanopore MinION sequencing data analysis.


*Journal: GigaScience* *Publication Date: 2019-Jan-24*

[https://academic.oup.com/gigascience/article-lookup/doi/10.1093/gigascience/giy169](https://academic.oup.com/gigascience/article-lookup/doi/10.1093/gigascience/giy169){: .btn}

*abstract:* The fast-moving progress of the third generation long read sequencing technologies will soon bring the biological and medical sciences to a new era of research. Altogether the technique and experimental procedures are becoming more straightforward and available to biologists from diverse fields, even without any profound experience in DNA sequencing. Thus, the introduction of the MinIONTM device by Oxford Nanopore Technologies promises to "bring sequencing technology to the masses" and also allows quick and operative analysis in field studies. However, the convenience of this sequencing technology dramatically contrasts with the available analysis tools, which may significantly reduce enthusiasm of a "regular" user. To really bring the sequencing technology to every biologist, we need a set of user-friendly tools that can perform a powerful analysis in an automatic manner.

## 9. Overview of the BioCreative VI Precision Medicine Track: mining protein interactions and mutations for precision medicine.


*Journal: Database : the journal of biological databases and curation* *Publication Date: 2019-Jan-01*

[https://academic.oup.com/database/article-lookup/doi/10.1093/database/bay147](https://academic.oup.com/database/article-lookup/doi/10.1093/database/bay147){: .btn}

*abstract:* The Precision Medicine Initiative is a multicenter effort aiming at formulating personalized treatments leveraging on individual patient data (clinical, genome sequence and functional genomic data) together with the information in large knowledge bases (KBs) that integrate genome annotation, disease association studies, electronic health records and other data types. The biomedical literature provides a rich foundation for populating these KBs, reporting genetic and molecular interactions that provide the scaffold for the cellular regulatory systems and detailing the influence of genetic variants in these interactions. The goal of BioCreative VI Precision Medicine Track was to extract this particular type of information and was organized in two tasks: (i) document triage task, focused on identifying scientific literature containing experimentally verified protein-protein interactions (PPIs) affected by genetic mutations and (ii) relation extraction task, focused on extracting the affected interactions (protein pairs). To assist system developers and task participants, a large-scale corpus of PubMed documents was manually annotated for this task. Ten teams worldwide contributed 22 distinct text-mining models for the document triage task, and six teams worldwide contributed 14 different text-mining systems for the relation extraction task. When comparing the text-mining system predictions with human annotations, for the triage task, the best F-score was 69.06%, the best precision was 62.89%, the best recall was 98.0% and the best average precision was 72.5%. For the relation extraction task, when taking homologous genes into account, the best F-score was 37.73%, the best precision was 46.5% and the best recall was 54.1%. Submitted systems explored a wide range of methods, from traditional rule-based, statistical and machine learning systems to state-of-the-art deep learning methods. Given the level of participation and the individual team results we find the precision medicine track to be successful in engaging the text-mining research community. In the meantime, the track produced a manually annotated corpus of 5509 PubMed documents developed by BioGRID curators and relevant for precision medicine. The data set is freely available to the community, and the specific interactions have been integrated into the BioGRID data set. In addition, this challenge provided the first results of automatically identifying PubMed articles that describe PPI affected by mutations, as well as extracting the affected relations from those articles. Still, much progress is needed for computer-assisted precision medicine text mining to become mainstream. Future work should focus on addressing the remaining technical challenges and incorporating the practical benefits of text-mining tools into real-world precision medicine information-related curation.

## 10. Machine learning algorithms estimating prognosis and guiding therapy in adult congenital heart disease: data from a single tertiary centre including 10 019 patients.


*Journal: European heart journal* *Publication Date: 2019-Jan-26*

[https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehy915](https://academic.oup.com/eurheartj/article-lookup/doi/10.1093/eurheartj/ehy915){: .btn}

*abstract:* To assess the utility of machine learning algorithms on estimating prognosis and guiding therapy in a large cohort of patients with adult congenital heart disease (ACHD) or pulmonary hypertension at a single, tertiary centre.

## 11. DeepAMR for predicting co-occurrent resistance of Mycobacterium tuberculosis.


*Journal: Bioinformatics (Oxford, England)* *Publication Date: 2019-Jan-28*

[https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz067](https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/btz067){: .btn}

*abstract:* Resistance co-occurrence within first-line anti-tuberculosis (TB) drugs is a common phenomenon. Existing methods based on genetic data analysis of Mycobacterium tuberculosis (MTB) have been able to predict resistance of MTB to individual drugs, but have not considered the resistance co-occurrence and cannot capture latent structure of genomic data that corresponds to lineages.

## 12. Reply to "Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists" by H. A. Haenssle et al.


*Journal: Annals of oncology : official journal of the European Society for Medical Oncology* *Publication Date: 2019-Jan-28*

[https://academic.oup.com/annonc/article-lookup/doi/10.1093/annonc/mdz015](https://academic.oup.com/annonc/article-lookup/doi/10.1093/annonc/mdz015){: .btn}

*abstract:* 

## 13. Predicting neurological recovery with Canonical Autocorrelation Embeddings.


*Journal: PloS one* *Publication Date: 2019*

[http://dx.plos.org/10.1371/journal.pone.0210966](http://dx.plos.org/10.1371/journal.pone.0210966){: .btn}

*abstract:* Early prediction of the potential for neurological recovery after resuscitation from cardiac arrest is difficult but important. Currently, no clinical finding or combination of findings are sufficient to accurately predict or preclude favorable recovery of comatose patients in the first 24 to 48 hours after resuscitation. Thus, life-sustaining therapy is often continued for several days in patients whose irrecoverable injury is not yet recognized. Conversely, early withdrawal of life-sustaining therapy increases mortality among patients who otherwise might have gone on to recover. In this work, we present Canonical Autocorrelation Analysis (CAA) and Canonical Autocorrelation Embeddings (CAE), novel methods suitable for identifying complex patterns in high-resolution multivariate data often collected in highly monitored clinical environments such as intensive care units. CAE embeds sets of datapoints onto a space that characterizes their latent correlation structures and allows direct comparison of these structures through the use of a distance metric. The methodology may be particularly suitable when the unit of analysis is not just an individual datapoint but a dataset, as for instance in patients for whom physiological measures are recorded over time, and where changes of correlation patterns in these datasets are informative for the task at hand. We present a proof of concept to illustrate the potential utility of CAE by applying it to characterize electroencephalographic recordings from 80 comatose survivors of cardiac arrest, aiming to identify patients who will survive to hospital discharge with favorable functional recovery. Our results show that with very low probability of making a Type 1 error, we are able to identify 32.5% of patients who are likely to have a good neurological outcome, some of whom have otherwise unfavorable clinical characteristics. Importantly, some of these had 5% predicted chance of favorable recovery based on initial illness severity measures alone. Providing this information to support clinical decision-making could motivate the continuation of life-sustaining therapies for these patients.

## 14. In silico identification of critical proteins associated with learning process and immune system for Down syndrome.


*Journal: PloS one* *Publication Date: 2019*

[http://dx.plos.org/10.1371/journal.pone.0210954](http://dx.plos.org/10.1371/journal.pone.0210954){: .btn}

*abstract:* Understanding expression levels of proteins and their interactions is a key factor to diagnose and explain the Down syndrome which can be considered as the most prevalent reason of intellectual disability in human beings. In the previous studies, the expression levels of 77 proteins obtained from normal genotype control mice and from trisomic Ts65Dn mice have been analyzed after training in contextual fear conditioning with and without injection of the memantine drug using statistical methods and machine learning techniques. Recent studies have also pointed out that there may be a linkage between the Down syndrome and the immune system. Thus, the research presented in this paper aim at in silico identification of proteins which are significant to the learning process and the immune system and to derive the most accurate model for classification of mice. In this paper, the features are selected by implementing forward feature selection method after preprocessing step of the dataset. Later, deep neural network, gradient boosting tree, support vector machine and random forest classification methods are implemented to identify the accuracy. It is observed that the selected feature subsets not only yield higher accuracy classification results but also are composed of protein responses which are important for the learning and memory process and the immune system.

## 15. Learning the sequence of influenza A genome assembly during viral replication using point process models and fluorescence in situ hybridization.


*Journal: PLoS computational biology* *Publication Date: 2019-Jan-28*

[http://dx.plos.org/10.1371/journal.pcbi.1006199](http://dx.plos.org/10.1371/journal.pcbi.1006199){: .btn}

*abstract:* Within influenza virus infected cells, viral genomic RNA are selectively packed into progeny virions, which predominantly contain a single copy of 8 viral RNA segments. Intersegmental RNA-RNA interactions are thought to mediate selective packaging of each viral ribonucleoprotein complex (vRNP). Clear evidence of a specific interaction network culminating in the full genomic set has yet to be identified. Using multi-color fluorescence in situ hybridization to visualize four vRNP segments within a single cell, we developed image-based models of vRNP-vRNP spatial dependence. These models were used to construct likely sequences of vRNP associations resulting in the full genomic set. Our results support the notion that selective packaging occurs during cytoplasmic transport and identifies the formation of multiple distinct vRNP sub-complexes that likely form as intermediate steps toward full genomic inclusion into a progeny virion. The methods employed demonstrate a statistically driven, model based approach applicable to other interaction and assembly problems.

## 16. Meta-Analysis of Nanoparticle Cytotoxicity via Data-Mining the Literature.
**Keywords:** **'cell viability', 'classification decision trees', 'machine learning', 'meta-analysis', 'nanoparticle cytotoxicity'**

*Journal: ACS nano* *Publication Date: 2019-Jan-31*

[https://dx.doi.org/10.1021/acsnano.8b07562](https://dx.doi.org/10.1021/acsnano.8b07562){: .btn}

*abstract:* Developing predictive modeling frameworks of potential cytotoxicity of engineered nanoparticles is critical for environmental and health risk analysis. The complexity and the heterogeneity of available data on potential risks of nanoparticles, in addition to interdependency of relevant influential attributes, makes it challenging to develop a generalization of nanoparticle toxicity behavior. Lack of systematic approaches to investigate these risks further adds uncertainties and variability to the body of literature and limits generalizability of existing studies. Here, we developed a rigorous approach for assembling published evidence on cytotoxicity of several organic and inorganic nanoparticles and unraveled hidden relationships that were not targeted in the original publications. We used a machine learning approach that employs decision trees together with feature selection algorithms ( e.g., Gain ratio) to analyze a set of published nanoparticle cytotoxicity sample data (2896 samples). The specific studies were selected because they specified nanoparticle-, cell-, and screening method-related attributes. The resultant decision-tree classifiers are sufficiently simple, accurate, and with high prediction power and should be widely applicable to a spectrum of nanoparticle cytotoxicity settings. Among several influential attributes, we show that the cytotoxicity of nanoparticles is primarily predicted from the nanoparticle material chemistry, followed by nanoparticle concentration and size, cell type, and cytotoxicity screening indicator. Overall, our study indicates that following rigorous and transparent methodological experimental approaches, in parallel to continuous addition to this data set developed using our approach, will offer higher predictive power and accuracy and uncover hidden relationships. Results obtained in this study help focus future studies to develop nanoparticles that are safe by design.

## 17. Recent Advances in Flexible and Wearable Pressure Sensors Based on Piezoresistive 3D Monolithic Conductive Sponges.


*Journal: ACS applied materials & interfaces* *Publication Date: 2019-Jan-28*

[https://dx.doi.org/10.1021/acsami.8b20929](https://dx.doi.org/10.1021/acsami.8b20929){: .btn}

*abstract:* High-performance flexible strain and pressure sensors are important components of the systems for human motion detection, human-machine interaction, soft robotics, electronic skin, etc., which are envisioned as the key technologies for applications in future human healthcare monitoring and artificial intelligence. In recent years, highly flexible and wearable strain/pressure sensors have been developed based on various materials/structures and transduction mechanisms. Piezoresistive three-dimensional (3D) monolithic conductive sponge, the resistance of which changes upon external pressure or stimuli, has emerged as a forefront material for flexible and wearable pressure sensor due to its excellent sensor performance, facile fabrication, and simple circuit integration. This review focuses on the rapid development of the piezoresistive pressure sensors based on 3D conductive sponges. Various piezoresistive conductive sponges are categorized into four different types and their material and structural characteristics are summarized. Methods for preparation of the 3D conductive sponges are reviewed, followed by examples of device performance and selected applications. The review concludes with a critical reflection of the current status and challenges. Prospects of the 3D conductive sponge for flexible and wearable pressure sensor are discussed.

## 18. Commissioning of a fluoroscopic-based real-time markerless tumor tracking system in a superconducting rotating gantry for carbon-ion pencil beam scanning treatment.
**Keywords:** **'image guidance', 'machine learning', 'markerless tracking', 'real-time image processing', 'rotating gantry', 'scanned carbon-ion beam'**

*Journal: Medical physics* *Publication Date: 2019-Jan-28*

[https://doi.org/10.1002/mp.13403](https://doi.org/10.1002/mp.13403){: .btn}

*abstract:* To perform the final quality assurance of our fluoroscopic-based markerless tumor tracking for gated carbon-ion pencil beam scanning (C-PBS) radiotherapy using a rotating gantry system, we evaluated the geometrical accuracy and tumor tracking accuracy using a moving chest phantom with simulated respiration.

## 19. Impact of a combination of quantitative indices representing uptake intensity, shape, and asymmetry in DAT SPECT using machine learning: comparison of different volume of interest settings.
**Keywords:** **'123I-FP-CIT', '123I-Ioflupane', 'DAT SPECT', 'Machine learning', 'Parkinson’s syndrome', 'Support vector machine'**

*Journal: EJNMMI research* *Publication Date: 2019-Jan-28*

[https://dx.doi.org/10.1186/s13550-019-0477-x](https://dx.doi.org/10.1186/s13550-019-0477-x){: .btn}

*abstract:* We sought to assess the machine learning-based combined diagnostic accuracy of three types of quantitative indices obtained using dopamine transporter single-photon emission computed tomography (DAT SPECT)-specific binding ratio (SBR), putamen-to-caudate ratio (PCR)/fractal dimension (FD), and asymmetry index (AI)-for parkinsonian syndrome (PS). We also aimed to compare the effect of two different types of volume of interest (VOI) settings from commercially available software packages DaTQUANT (Q) and DaTView (V) on diagnostic accuracy.

## 20. Preoperative prediction of microvascular invasion in hepatocellular cancer: a radiomics model using Gd-EOB-DTPA-enhanced MRI.
**Keywords:** **'Gd-EOB-DTPA', 'Hepatocellular cancer', 'Magnetic resonance imaging', 'Radiomics'**

*Journal: European radiology* *Publication Date: 2019-Jan-28*

[https://dx.doi.org/10.1007/s00330-018-5935-8](https://dx.doi.org/10.1007/s00330-018-5935-8){: .btn}

*abstract:* Preoperative prediction of microvascular invasion (MVI) in patients with hepatocellular cancer (HCC) is important for surgery strategy making. We aimed to develop and validate a combined intratumoural and peritumoural radiomics model based on gadolinium-ethoxybenzyl-diethylenetriamine (Gd-EOB-DTPA)-enhanced magnetic resonance imaging (MRI) for preoperative prediction of MVI in primary HCC patients.

## 21. Nurses "Seeing Forest for the Trees" in the Age of Machine Learning: Using Nursing Knowledge to Improve Relevance and Performance.


*Journal: Computers, informatics, nursing : CIN* *Publication Date: 2019-Jan-25*

[http://dx.doi.org/10.1097/CIN.0000000000000508](http://dx.doi.org/10.1097/CIN.0000000000000508){: .btn}

*abstract:* Although machine learning is increasingly being applied to support clinical decision making, there is a significant gap in understanding what it is and how nurses should adopt it in practice. The purpose of this case study is to show how one application of machine learning may support nursing work and to discuss how nurses can contribute to improving its relevance and performance. Using data from 130 specialized hospitals with 101 766 patients with diabetes, we applied various advanced statistical methods (known as machine learning algorithms) to predict early readmission. The best-performing machine learning algorithm showed modest predictive ability with opportunities for improvement. Nurses can contribute to machine learning algorithms by (1) filling data gaps with nursing-relevant data that provide personalized context about the patient, (2) improving data preprocessing techniques, and (3) evaluating potential value in practice. These findings suggest that nurses need to further process the information provided by machine learning and apply "Wisdom-in-Action" to make appropriate clinical decisions. Nurses play a pivotal role in ensuring that machine learning algorithms are shaped by their unique knowledge of each patient's personalized context. By combining machine learning with unique nursing knowledge, nurses can provide more visibility to nursing work, advance nursing science, and better individualize patient care. Therefore, to successfully integrate and maximize the benefits of machine learning, nurses must fully participate in its development, implementation, and evaluation.

## 22. HIV and age underlie specific patterns of brain abnormalities and cognitive changes in high functioning patients.


*Journal: Neuropsychology* *Publication Date: 2019-Jan-28*



*abstract:* Findings on the influence of age and HIV on brain and cognition remain equivocal, particularly in aviremic subjects without other age or HIV-related comorbidities. We aimed to (a) examine the effect of HIV status and age on structural brain measurements and cognition, and (b) apply the machine learning technique to identify brain morphometric and cognitive features that are most discriminative between aviremic subjects with HIV on stable combination antiretroviral therapy (cART) and healthy controls.

## 23. A Combined Drug Discovery Strategy Based on Machine Learning and Molecular Docking.
**Keywords:** **'ACC inhibitors', 'Extremely randomized trees', 'Machine learning', 'Molecular docking'**

*Journal: Chemical biology & drug design* *Publication Date: 2019-Jan-28*

[https://doi.org/10.1111/cbdd.13494](https://doi.org/10.1111/cbdd.13494){: .btn}

*abstract:* Data mining methods based on machine learning play an increasingly important role in drug design and discovery. In the current work, eight machine learning methods including decision trees, k-Nearest neighbor, support vector machines, random forests, extremely randomized trees, AdaBoost, gradient boosting trees, and XGBoost were evaluated comprehensively through a case study of ACC inhibitor data sets. Internal and external data sets were employed for cross-validation of the eight machine learning methods. Results showed that the extremely randomized trees model performed best and was adopted as the first step of virtual screening. Together with structure-based virtual screening in the second step, this combined strategy obtained desirable results. This work indicates that the combination of machine learning methods with traditional structure-based virtual screening can effectively strengthen the ability in finding potential hits from large compound database for a given target. This article is protected by copyright. All rights reserved.

## 24. Chalcogenide Phase Change Material for Active Terahertz Photonics.
**Keywords:** **'germanium antimony telluride', 'metamaterials', 'non-volatile photonics', 'photonics', 'terahertz', 'ultrafast modulators'**

*Journal: Advanced materials (Deerfield Beach, Fla.)* *Publication Date: 2019-Jan-27*

[https://doi.org/10.1002/adma.201808157](https://doi.org/10.1002/adma.201808157){: .btn}

*abstract:* The strikingly contrasting optical properties of various phases of chalcogenide phase change materials (PCM) has recently led to the development of novel photonic devices such as all-optical non-von Neumann memory, nanopixel displays, color rendering, and reconfigurable nanoplasmonics. However, the exploration of chalcogenide photonics is currently limited to optical and infrared frequencies. Here, a phase change material integrated terahertz metamaterial for multilevel nonvolatile resonance switching with spatial and temporal selectivity is demonstrated. By controlling the crystalline proportion of the PCM film, multilevel, non-volatile, terahertz resonance switching states with long retention time at zero hold power are realized. Spatially selective reconfiguration at sub-metamaterial scale is shown by delivering electrical stimulus locally through designer interconnect architecture. The PCM metamaterial also features ultrafast optical modulation of terahertz resonances with tunable switching speed based on the crystalline order of the PCM film. The multilevel nonvolatile, spatially selective, and temporally tunable PCM metamaterial will provide a pathway toward development of novel and disruptive terahertz technologies including spatio-temporal terahertz modulators for high speed wireless communication, neuromorphic photonics, and machine-learning metamaterials.

## 25. In silico prediction of chemical reproductive toxicity using machine learning.
**Keywords:** **'machine learning', 'molecular fingerprint', 'reproductive toxicity', 'structural alerts', 'structure-activity relationship'**

*Journal: Journal of applied toxicology : JAT* *Publication Date: 2019-Jan-27*

[https://doi.org/10.1002/jat.3772](https://doi.org/10.1002/jat.3772){: .btn}

*abstract:* Reproductive toxicity is an important regulatory endpoint in health hazard assessment. Because the in vivo tests are expensive, time consuming and require a large number of animals, which must be killed, in silico approaches as the alternative strategies have been developed to assess the potential reproductive toxicity (reproductive toxicity) of chemicals. Some prediction models for reproductive toxicity have been developed, but most of them were built only based on one single endpoint such as embryo teratogenicity; therefore, these models may not provide reliable predictions for toxic chemicals with other endpoints, such as sperm reduction or gonadal dysgenesis. Here, a total of 1823 chemicals for reproductive toxicity characterized by multiple endpoints were used to develop structure-activity relationship models by six machine-learning approaches with nine molecular fingerprints. Among the models, MACCSFP-SVM model has the best performance for the external validation set (area under the curve = 0.900, classification accuracy = 0.836). The applicability domain was analyzed, and a rational boundary was found to distinguish inaccurate predictions and accurate predictions. Moreover, several structural alerts for characterizing reproductive toxicity were identified using the information gain combining substructure frequency analysis. Our results would be helpful for the prediction of the reproductive toxicity of chemicals.

## 26. Natural language generation for electronic health records.


*Journal: NPJ digital medicine* *Publication Date: 2018-Nov-19*



*abstract:* One broad goal of biomedical informatics is to generate fully-synthetic, faithfully representative electronic health records (EHRs) to facilitate data sharing between healthcare providers and researchers and promote methodological research. A variety of methods existing for generating synthetic EHRs, but they are not capable of generating unstructured text, like emergency department (ED) chief complaints, history of present illness, or progress notes. Here, we use the encoder-decoder model, a deep learning algorithm that features in many contemporary machine translation systems, to generate synthetic chief complaints from discrete variables in EHRs, like age group, gender, and discharge diagnosis. After being trained end-to-end on authentic records, the model can generate realistic chief complaint text that appears to preserve the epidemiological information encoded in the original record-sentence pairs. As a side effect of the model's optimization goal, these synthetic chief complaints are also free of relatively uncommon abbreviation and misspellings, and they include none of the personally identifiable information (PII) that was in the training data, suggesting that this model may be used to support the de-identification of text in EHRs. When combined with algorithms like generative adversarial networks (GANs), our model could be used to generate fully-synthetic EHRs, allowing healthcare providers to share faithful representations of multimodal medical data without compromising patient privacy. This is an important advance that we hope will facilitate the development of machine-learning methods for clinical decision support, disease surveillance, and other data-hungry applications in biomedical informatics.

## 27. Geoffrey: An Automated Schedule System on a Social Robot for the Intellectually Challenged.


*Journal: Computational intelligence and neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.1155/2018/4350272](https://dx.doi.org/10.1155/2018/4350272){: .btn}

*abstract:* The accelerated growth of the percentage of elder people and persons with brain injury-related conditions and who are intellectually challenged are some of the main concerns of the developed countries. These persons often require special cares and even almost permanent overseers that help them to carry out diary tasks. With this issue in mind, we propose an automated schedule system which is deployed on a social robot. The robot keeps track of the tasks that the patient has to fulfill in a diary basis. When a task is triggered, the robot guides the patient through its completion. The system is also able to detect if the steps are being properly carried out or not, issuing alerts in that case. To do so, an ensemble of deep learning techniques is used. The schedule is customizable by the carers and authorized relatives. Our system could enhance the quality of life of the patients and improve their self-autonomy. The experimentation, which was supervised by the ADACEA foundation, validates the achievement of these goals.

## 28. Prediction of Conversion From Amnestic Mild Cognitive Impairment to Alzheimer's Disease Based on the Brain Structural Connectome.
**Keywords:** **'brain network', 'conversion', 'diffusion tensor imaging', 'graph theory', 'machine learning', 'mild cognitive impairment'**

*Journal: Frontiers in neurology* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fneur.2018.01178](https://dx.doi.org/10.3389/fneur.2018.01178){: .btn}

*abstract:* 

## 29. Bio-inspired Analysis of Deep Learning on Not-So-Big Data Using Data-Prototypes.
**Keywords:** **'data prototypes', 'deep learning', 'interpretability', 'not so big data', 'transfer learning'**

*Journal: Frontiers in computational neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fncom.2018.00100](https://dx.doi.org/10.3389/fncom.2018.00100){: .btn}

*abstract:* Deep artificial neural networks are feed-forward architectures capable of very impressive performances in diverse domains. Indeed stacking multiple layers allows a hierarchical composition of local functions, providing efficient compact mappings. Compared to the brain, however, such architectures are closer to a single pipeline and require huge amounts of data, while concrete cases for either human or machine learning systems are often restricted to not-so-big data sets. Furthermore, interpretability of the obtained results is a key issue: since deep learning applications are increasingly present in society, it is important that the underlying processes be accessible and understandable to every one. In order to target these challenges, in this contribution we analyze how considering prototypes in a rather generalized sense (with respect to the state of the art) allows to reasonably work with small data sets while providing an interpretable view of the obtained results. Some mathematical interpretation of this proposal is discussed. Sensitivity to hyperparameters is a key issue for reproducible deep learning results, and is carefully considered in our methodology. Performances and limitations of the proposed setup are explored in details, under different hyperparameter sets, in an analogous way as biological experiments are conducted. We obtain a rather simple architecture, easy to explain, and which allows, combined with a standard method, to target both performances and interpretability.

## 30. Machine Learning Models for Multiparametric Glioma Grading With Quantitative Result Interpretations.
**Keywords:** **'digital pathology images', 'glioma grading', 'machine learning', 'morphological features', 'support vector machine'**

*Journal: Frontiers in neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fnins.2018.01046](https://dx.doi.org/10.3389/fnins.2018.01046){: .btn}

*abstract:* Gliomas are the most common primary malignant brain tumors in adults. Accurate grading is crucial as therapeutic strategies are often disparate for different grades and may influence patient prognosis. This study aims to provide an automated glioma grading platform on the basis of machine learning models. In this paper, we investigate contributions of multi-parameters from multimodal data including imaging parameters or features from the Whole Slide images (WSI) and the proliferation marker Ki-67 for automated brain tumor grading. For each WSI, we extract both visual parameters such as morphology parameters and sub-visual parameters including first-order and second-order features. On the basis of machine learning models, our platform classifies gliomas into grades II, III, and IV. Furthermore, we quantitatively interpret and reveal the important parameters contributing to grading with the Local Interpretable Model-Agnostic Explanations (LIME) algorithm. The quantitative analysis and explanation may assist clinicians to better understand the disease and accordingly to choose optimal treatments for improving clinical outcomes. The performance of our grading model was evaluated with cross-validation, which randomly divided the patients into non-overlapping training and testing sets and repeatedly validated the model on the different testing sets. The primary results indicated that this modular platform approach achieved the highest grading accuracy of 0.90 ± 0.04 with support vector machine (SVM) algorithm, with grading accuracies of 0.91 ± 0.08, 0.90 ± 0.08, and 0.90 ± 0.07 for grade II, III, and IV gliomas, respectively.

## 31. Topological Properties of Resting-State fMRI Functional Networks Improve Machine Learning-Based Autism Classification.
**Keywords:** **'ABIDE', 'SVM–support vector machine', 'brain connectitvity', 'fMRI', 'graph theoiy', 'machine learing'**

*Journal: Frontiers in neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fnins.2018.01018](https://dx.doi.org/10.3389/fnins.2018.01018){: .btn}

*abstract:* Automatic algorithms for disease diagnosis are being thoroughly researched for use in clinical settings. They usually rely on pre-identified biomarkers to highlight the existence of certain problems. However, finding such biomarkers for neurodevelopmental disorders such as Autism Spectrum Disorder (ASD) has challenged researchers for many years. With enough data and computational power, machine learning (ML) algorithms can be used to interpret the data and extract the best biomarkers from thousands of candidates. In this study, we used the fMRI data of 816 individuals enrolled in the Autism Brain Imaging Data Exchange (ABIDE) to introduce a new biomarker extraction pipeline for ASD that relies on the use of graph theoretical metrics of fMRI-based functional connectivity to inform a support vector machine (SVM). Furthermore, we split the dataset into 5 age groups to account for the effect of aging on functional connectivity. Our methodology achieved better results than most state-of-the-art investigations on this dataset with the best model for the >30 years age group achieving an accuracy, sensitivity, and specificity of 95, 97, and 95%, respectively. Our results suggest that measures of centrality provide the highest contribution to the classification power of the models.

## 32. Clinical Value of Machine Learning in the Automated Detection of Focal Cortical Dysplasia Using Quantitative Multimodal Surface-Based Features.
**Keywords:** **'focal cortical dysplasia', 'machine learning', 'metabolic', 'morphological', 'quantitative'**

*Journal: Frontiers in neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fnins.2018.01008](https://dx.doi.org/10.3389/fnins.2018.01008){: .btn}

*abstract:* 

## 33. Longitudinal Connectomes as a Candidate Progression Marker for Prodromal Parkinson's Disease.
**Keywords:** **'diffusion magnetic resonance imaging', 'longitudinal connectomes', 'machine learning', 'neurodegeneration', 'prodromal Parkinson’s disease'**

*Journal: Frontiers in neuroscience* *Publication Date: 2018*

[https://dx.doi.org/10.3389/fnins.2018.00967](https://dx.doi.org/10.3389/fnins.2018.00967){: .btn}

*abstract:* Parkinson's disease is the second most prevalent neurodegenerative disorder in the Western world. It is estimated that the neuronal loss related to Parkinson's disease precedes the clinical diagnosis by more than 10 years (prodromal phase) which leads to a subtle decline that translates into non-specific clinical signs and symptoms. By leveraging diffusion magnetic resonance imaging brain (MRI) data evaluated longitudinally, at least at two different time points, we have the opportunity of detecting and measuring brain changes early on in the neurodegenerative process, thereby allowing early detection and monitoring that can enable development and testing of disease modifying therapies. In this study, we were able to define a longitudinal degenerative Parkinson's disease progression pattern using diffusion magnetic resonance imaging connectivity information. Such pattern was discovered using a 

## 34. Modeling the health effects of time-varying complex environmental mixtures: Mean field variational Bayes for lagged kernel machine regression.
**Keywords:** **'Bayesian inference', 'child health', 'environmental health', 'longitudinal data', 'machine learning', 'mixtures'**

*Journal: Environmetrics* *Publication Date: 2018-Jun*



*abstract:* There is substantial interest in assessing how exposure to environmental mixtures, such as chemical mixtures, affect child health. Researchers are also interested in identifying critical time windows of susceptibility to these complex mixtures. A recently developed method, called lagged kernel machine regression (LKMR), simultaneously accounts for these research questions by estimating effects of time-varying mixture exposures, and identifying their critical exposure windows. However, LKMR inference using Markov chain Monte Carlo methods (MCMC-LKMR) is computationally burdensome and time intensive for large datasets, limiting its applicability. Therefore, we develop a mean field variational Bayesian inference procedure for lagged kernel machine regression (MFVB-LKMR). The procedure achieves computational efficiency and reasonable accuracy as compared with the corresponding MCMC estimation method. Updating parameters using MFVB may only take minutes, while the equivalent MCMC method may take many hours or several days. We apply MFVB-LKMR to PROGRESS, a prospective cohort study in Mexico. Results from a subset of PROGRESS using MFVB-LKMR provide evidence of significant positive association between second trimester cobalt levels and z-scored birthweight. This positive association is heightened by cesium exposure. MFVB-LKMR is a promising approach for computationally efficient analysis of environmental health datasets, to identify critical windows of exposure to complex mixtures.

## 35. Artificial Intelligence in Radiology: Resident Recruitment Help or Hindrance?


*Journal: Academic radiology* *Publication Date: 2019-Jan-24*

[https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30026-1](https://linkinghub.elsevier.com/retrieve/pii/S1076-6332(19)30026-1){: .btn}

*abstract:* 

## 36. "Artificial intelligence": Which services, which applications, which results and which development today in clinical research? Which impact on the quality of care? Which recommendations?
**Keywords:** **'AI', 'Assessment', 'Clinical research', 'Clinical trials', 'Data', 'Governance', 'Interdisciplinary', 'Interoperability', 'Knowledge', 'Real-life studies', 'Training'**

*Journal: Therapie* *Publication Date: 2018-Dec-13*



*abstract:* Artificial intelligence (AI), beyond the concrete applications that have already become part of our daily lives, makes it possible to process numerous and heterogeneous data and knowledge, and to understand potentially complex and abstract rules in a manner human intelligence can but without human intervention. AI combines two properties, self-learning by the successive and repetitive processing of data as well as the capacity to adapt, that is to say the possibility for a scripted program to deal with multiple situations likely to vary over time. Roundtable experts confirmed the potential contribution and theoretical benefit of AI in clinical research and in improving the efficiency of patient care. Experts also measured, as is the case for any new process that people need to get accustomed to, its impact on practices and mindset. To maximize the benefits of AI, four critical points have been identified. The careful consideration of these four points conditions the technical integration and the appropriation by all actors of the life science spectrum: researchers, regulators, drug developers, care establishments, medical practitioners and, above all, patients and the civil society. 1st critical point: produce tangible demonstrations of the contributions of AI in clinical research by quantifying its benefits. 2nd critical point: build trust to foster dissemination and acceptability of AI in healthcare thanks to an adapted regulatory framework. 3rd critical point: ensure the availability of technical skills, which implies an investment in training, the attractiveness of the health sector relative to tech-heavy sectors and the development of ergonomic data collection tools for all health operators. 4th critical point: organize a system of governance for a distributed and secure model at the national level to aggregate the information and services existing at the local level. Thirty-seven concrete recommendations have been formulated which should pave the way for a widespread adoption of AI in clinical research. In this context, the French "Health data hub" initiative constitutes an ideal opportunity.

## 37. Reprint of: Mapping human brain lesions and their functional consequences.
**Keywords:** **'Cognitive neurology', 'Human', 'Lesion analysis', 'MLBM', 'Machine learning', 'Mass-univariate', 'Multivariate lesion behavior mapping', 'Network', 'Neuroanatomy', 'Neuropsychology', 'Non-parametric mapping', 'Stroke', 'VLBM', 'VLSM', 'Voxel-based lesion symptom mapping'**

*Journal: NeuroImage* *Publication Date: 2019-Jan-25*

[https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(19)30045-X](https://linkinghub.elsevier.com/retrieve/pii/S1053-8119(19)30045-X){: .btn}

*abstract:* Neuroscience has a long history of inferring brain function by examining the relationship between brain injury and subsequent behavioral impairments. The primary advantage of this method over correlative methods is that it can tell us if a certain brain region is necessary for a given cognitive function. In addition, lesion-based analyses provide unique insights into clinical deficits. In the last decade, statistical voxel-based lesion behavior mapping (VLBM) emerged as a powerful method for understanding the architecture of the human brain. This review illustrates how VLBM improves our knowledge of functional brain architecture, as well as how it is inherently limited by its mass-univariate approach. A wide array of recently developed methods appear to supplement traditional VLBM. This paper provides an overview of these new methods, including the use of specialized imaging modalities, the combination of structural imaging with normative connectome data, as well as multivariate analyses of structural imaging data. We see these new methods as complementing rather than replacing traditional VLBM, providing synergistic tools to answer related questions. Finally, we discuss the potential for these methods to become established in cognitive neuroscience and in clinical applications.

## 38. A gentle introduction to deep learning in medical image processing.
**Keywords:** **'Computer-aided diagnosis', 'Deep learning', 'Image reconstruction', 'Image registration', 'Image segmentation', 'Introduction', 'Machine learning', 'Physical simulation'**

*Journal: Zeitschrift fur medizinische Physik* *Publication Date: 2019-Jan-24*

[https://linkinghub.elsevier.com/retrieve/pii/S0939-3889(18)30120-X](https://linkinghub.elsevier.com/retrieve/pii/S0939-3889(18)30120-X){: .btn}

*abstract:* This paper tries to give a gentle introduction to deep learning in medical image processing, proceeding from theoretical foundations to applications. We first discuss general reasons for the popularity of deep learning, including several major breakthroughs in computer science. Next, we start reviewing the fundamental basics of the perceptron and neural networks, along with some fundamental theory that is often omitted. Doing so allows us to understand the reasons for the rise of deep learning in many application domains. Obviously medical image processing is one of these areas which has been largely affected by this rapid progress, in particular in image detection and recognition, image segmentation, image registration, and computer-aided diagnosis. There are also recent trends in physical simulation, modeling, and reconstruction that have led to astonishing results. Yet, some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results. These apparent weaknesses highlight current limitations of deep ()learning. However, we also briefly discuss promising approaches that might be able to resolve these problems in the future.

## 39. Mapping the Delirium Literature Through Probabilistic Topic Modeling and Network Analysis: A Computational Scoping Review.
**Keywords:** **'Delirium', 'acute confusional state', 'machine learning', 'network analysis', 'probabilistic topic modeling', 'scoping review'**

*Journal: Psychosomatics* *Publication Date: 2018-Dec-22*

[https://linkinghub.elsevier.com/retrieve/pii/S0033-3182(18)30520-6](https://linkinghub.elsevier.com/retrieve/pii/S0033-3182(18)30520-6){: .btn}

*abstract:* Delirium is an acute confusional state, associated with morbidity and mortality in diverse medically-ill populations. Delirium is recognized, through both professional competencies and instructional materials, as a core topic in consultation psychiatry.

## 40. Data Science for Child Health.
**Keywords:** **'classification', 'clinical', 'data mining', 'data warehousing', 'decision support systems', 'forecasting', 'machine learning', 'neural networks', 'phenotype'**

*Journal: The Journal of pediatrics* *Publication Date: 2019-Jan-24*

[https://linkinghub.elsevier.com/retrieve/pii/S0022-3476(18)31815-8](https://linkinghub.elsevier.com/retrieve/pii/S0022-3476(18)31815-8){: .btn}

*abstract:* 

